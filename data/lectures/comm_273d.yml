#####################################################################
"2014-09-23":
  topics:
  - Count something interesting
  - Make friends with math
  - The joy of text
  - How to do a data project
  homework:
    - title: Create a Github account and publish a document in Markdown
      description: |
        - Sign up for a [Github account](https://github.com/join).
        - Email me the name of your Github account.
        - Create a **New Repository** and name it however you'd like ([some tips here](https://guides.github.com/activities/hello-world/#repository)).
        - Create a new README.md file. Write anything you like, but do it in [Markdown](https://help.github.com/articles/markdown-basics)
        - Note: This repo will be **public**. As soon as you email me the name of your Github account, I'll set you up with a private repo, to which you can post homework assignments. Don't post homework assignments to your **public** repo (unless you want to, in which case, bravo).

    - title: Critique a piece of data journalism
      description: |
        Find a story that purports to use data, read it, and answer the following questions:

        - What are the datasets used here, and where did they come from?
        - What is the "birth story" of the data, e.g. how were they created in the first place?
        - What claims does the story make based on the data?
        - What are the limitations of the data?
        - Can you find where this (original) data exists online? If so, post the relevant URLs.
        Write this memo in Markdown format and post it to your __private__ Github repo. If this hasn't been set up for you, then just email it to me.

        [Here's an example memo](https://github.com/public-affairs-data-journalism/class_website/blob/master/source/pages/homework/mccann-data-critique.md). View it **[raw](https://raw.githubusercontent.com/public-affairs-data-journalism/class_website/master/source/pages/homework/mccann-data-critique.md)** on Github to see the actual Markdown.

        Here are examples of data-based stories that you can use for this assignment if you don't want to look for your own:

        - [Drugging Our Kids](http://webspecial.mercurynews.com/druggedkids/?page=pt1) - San Jose Mercury News
        - [Unseen Toll: Wages of Millions Seized to Pay Past Debts](http://www.propublica.org/article/unseen-toll-wages-of-millions-seized-to-pay-past-debts) - ProPublica
        - [When Caregivers Harm:](http://www.propublica.org/article/when-caregivers-harm-california-problem-nurses-stay-on-job-710) - ProPublica
        - [Water's edge: the crisis of rising sea levels](http://www.reuters.com/investigates/special-report/waters-edge-the-crisis-of-rising-sea-levels/#article-1-insidious-invasion) - Reuters
        - [Children and Guns: The Hidden Toll](http://www.nytimes.com/2013/09/29/us/children-and-guns-the-hidden-toll.html) New York Times
        - [Sliver of Medicare Doctors Get Big Share of Payouts](http://www.nytimes.com/2014/04/09/business/sliver-of-medicare-doctors-get-big-share-of-payouts.html) - New York Times
        - [The Undeserving Poor](http://www.reuters.com/subjects/income-inequality/indiana) - Reuters
        - [Stop and Seize](http://www.washingtonpost.com/sf/investigative/2014/09/06/stop-and-seize/) - Washington Post
        - [LAPD misclassified nearly 1,200 violent crimes as minor offenses](http://www.latimes.com/local/la-me-crimestats-lapd-20140810-story.html) - Los Angeles Times
        - [Child-care scams rake in thousands](http://www.jsonline.com/watchdog/watchdogreports/38283494.html) - Milwaukee Journal Sentinel
        - [‘Stop-and-Frisk’ Is All but Gone From New York](http://www.nytimes.com/interactive/2014/09/19/nyregion/stop-and-frisk-is-all-but-gone-from-new-york.html) - New York Times

    - title: List your 10 favorite restaurants in a CSV file
      description: |
        Create a [comma-seperated value (CSV) file](http://en.wikipedia.org/wiki/Comma-separated_values) with your 10 favorite restaurants. Include these fields: name,address,city,state,category,yelp_url

        Send it to me by email or post it to your private Github repo.

    - title: List 10 news-related Twitter accounts that you find interesting
      description: |
        Make a list of the Twitter account names (or their URLs), one account per line. Send it to me via email or post it on Github as a text file.




# ----------------------------# ------------# ----------------------------# ------------------


"2014-09-25": # "Bad Data" / making comparisons
  topics:
    - Fighting bad data with bad data
    - "Baltimore's declining rape statistics"
    - FBI crime reporting
    - The Uber effect on drunk driving
    - Pivot tables

  homework:
    - title: Investigate Uber's purported impact on SF's DUI reports
      description: |

         Note: I've created a video walkthrough on using pivot tables for this data. [Check it out here](/tutorials/spreadsheets/basic-pivot-tables).

         In class, we started to examine a claim by Uber that its ride-sharing service has likely *caused* a decrease in drunk-driving behavior. One of their datasets is the San Francisco Police Department's incident reports categorized as "driving under the influence."

         I've created another SFPD dataset: [all the incidents from 2003 to 2013 that were categorized as DUIs, "DRUNKENNESS", or "KIDNAPPINGS"](https://docs.google.com/a/stanford.edu/spreadsheets/d/1H2vQAZTG8eihyw_yzsI_sJfQBgxRn1fE1dOTV5E_W9M/edit#gid=1704088313).

         Using the mathematical reasoning we examined in the [Baltimore Sun's coverage of declining rape statistics](http://www.baltimoresun.com/news/maryland/crime/blog/bal-city-rape-statistics-archive,0,7495701.special), use this dataset to either support or cast doubt on Uber's analysis.

         In other words, your analysis must be able to complete this statement:

         > If it were true that Uber's launch in [San Francisco in June 2010](http://en.wikipedia.org/wiki/Uber_(company)) had a noticable impact on DUI reports, then we expect to see (*this or that*) in (*yadda yadda yadda*) kind of comparison. As the data shows, we (*do/do not*).

         Hint: Read the [Washington Post Wonkblog analysis](http://www.washingtonpost.com/blogs/wonkblog/wp/2014/07/10/are-uber-and-lyft-responsible-for-reducing-duis/) and note what they did not fully investigate.

    - title: Explore a snapshot of SF crime incident reports from 2014 with pivot tables
      description: |
        The San Francisco Police Department posts all of their [crime incident reports online](https://data.sfgov.org/Public-Safety/SFPD-Reported-Incidents-2003-to-Present/dyj4-n68b). I've create a couple of 2014 snapshots: [January to March](https://docs.google.com/a/stanford.edu/spreadsheets/d/1tSc2ivROr9Gj6h27wU5THgPmiA3VrcfSPNVD_ravLqE/edit?usp=drive_web) and [May to July](https://docs.google.com/a/stanford.edu/spreadsheets/d/1KazjqpLEjcjiVFQaPLn-zbAF9GHTUrPPR7X73Pv1liQ/).

        Make copies of one or the other or both. And then just explore them. In class, we looked at raw counts of _categories_ of incidents, such as DUIs. But there's more to the data, including subcategories of the incidents and how they were resolved. Examine a category of crime. Slice it up by time or date intervals. Make a pivot chart and even a visualization if you'd like. Then write up a quick one-pager describing to me what you did and anything you noticed.

    - title: Read several essays on tracking homicides and shootings
      description: |
        Read the following essays, turn in a one-page report answering these questions:

        1. Why is there no comprehensive and official data source on either number of homicides, gun-related homicides, or police-related homicides?
        2. What official sources exist and what are their limitations? And why do those limitations exist?
        3. How have independent researchers been able to collect data?
        4. What obstacles have they run into?

        The essays:
        - [What Slate learned from trying, and failing, to record every death by gun in America since Newtown.](http://www.slate.com/articles/news_and_politics/crime/2013/12/newtown_anniversary_what_slate_learned_from_trying_and_failing_to_record.single.html)
        - ["Homicide Watch: An Interview"](http://contentsmagazine.com/articles/homicide-watch-an-interview/)
        - [Nobody Knows How Many Americans The Police Kill Each Year](http://fivethirtyeight.com/features/how-many-americans-the-police-kill-each-year/)
        - [Why the Fatal Encounters project exists](http://www.fatalencounters.org/why-fe-exists2/)

    - title: "Read The New Precision Journalism, Chapter 8: Databases"
      description: |
        Read from the 1991 edition online: [http://www.unc.edu/~pmeyer/book/Chapter8.htm](http://www.unc.edu/~pmeyer/book/Chapter8.htm). No writeup necessary, but you should probably read it rather than risk being confused in the next class.



# ----------------------------# ------------# ----------------------------# ------------------



"2014-09-30":
# DIY Databases
  topics:
    - The importance of spreadsheets
    - Counting murders
    - Making calls
    - A crowdsourced spreadsheet

  homework:
    - title: Create a data memo for your beat
      description: |
        Similar to what you had to do for your other classes, draft a memo that is focused on the kinds of data you either *know* exists on your beat, or that you hope to find, or to collect yourself.

        List **five different sources** (actual or hopeful) as well as:

        - Why this data source is interesting and what you're curious about.
        - Where this data exists, or where you expect to find it
        - Anticipated problems in collecting or analyzing it

        This is **not a final draft**. I just want you to start thinking of data as soon as possible, and we'll work on refining and researching the possibilities in the next couple of weeks. Mostly, this is less about your grasp of data work than about the research you've done so far on your beat.

        ##### How to submit
        In your **private Github repo**, create a new file named "draft-data-beat-memo.md" (yes, it will be a Markdown file).

    - title: Read Phillip Reese's stories and write a data critique on Phillip Reese's stories
      description: |
        We're having a guest speaker on Tuesday: Phillip Reese, the computer-assisted reporting genius at the Sacramento Bee. He and his colleague, Cynthia Hubert, were [finalists for the 2014 Pulitzer in Investigative Reporting](http://www.pulitzer.org/2014_investigative_reporting_finalist_2) for their reporting on a Las Vegas mental hospital that bused more than 1,500 psychiatric patients out to 48 states in 5 years.

        Read **two** of the following stories and write up at least five questions you have about the reporting or data analysis process and prepare to challenge Phillip:

        - [Unsolved killings in South Sacramento](https://docs.google.com/file/d/0B6IBlUS1jXOuQmhleGgyNHJ2eE0/edit?usp=sharing)
        - [Busing of patients rife in Nevada](http://www.pulitzer.org/files/2014/investigative-reporting/sacbee/02sacbee2014.pdf)
        - [Million-dollar hospital bills rise sharply in Northern California](http://www.sacbee.com/2012/03/11/4328036/million-dollar-hospital-bills.html)
        - [Home, fire crews in peril](https://docs.google.com/file/d/0B6IBlUS1jXOuZUFMYVNCQXd4Mm8/edit)

        ##### How to submit
        In your **private Github repo**, create a new file named "phillip-reese-questions.md" (yes, it will be a Markdown file).

    - title: Practice pivot tables with homicide data
      description: |
        Using the homicide-related datasets we looked at in class, practice exploring them with pivot tables, write a quick analysis of what you tried and what you found. It's OK if the data seems difficult to summarize, and if that's the case, explain the problems you ran into (because these problems are the same as ones we'll be seeing in other datasets).

        Here's the Google spreadsheets that you can **copy** and then pivot on:

        - [Fatal Encounters, as submitted through its web form](https://docs.google.com/a/stanford.edu/spreadsheets/d/1CxbUHNQ99EiN5eOflPnEATRzz-lNtuSlrgk70Ar7LD4/edit#gid=0)
        - [Deadspin's list of police-involved deaths](https://docs.google.com/a/stanford.edu/spreadsheets/d/18g6cBZh30e1ydk-XXSoY1tbOODhBTmoXJzEu5aB47c8/edit#gid=1144428085)
        - [Slate's collection of gun-related deaths for 2013](https://docs.google.com/a/stanford.edu/spreadsheets/d/1DLEnoeN-OYDYw6EfwGqX8HnrlnZgJT72zGfQZ3KI3Po/edit#gid=0)

        Note: In Google Spreadsheets, you can make a copy of the spreadsheet by going to **File->Make a Copy...**. Or, if you want to view it in Excel, select **File->Download as...** and you can choose CSV or Excel format.

        ##### How to submit
        In your **private Github repo**, create a new file named "homicide-data-analysis.md" (yes, it will be a Markdown file).



# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-02":
  topics:
    - Phillip Reese speaks
  homework:
    - title: Create a Github Pages repository
      description: |
        Time to create a new repository. [Follow the tutorial here](http://www.padjo.org/tutorials/github/pages-setup/) Email me if you're running into problems. You don't have to make some special just make something and make it visible on the web.
    - title: Find 10 interesting online maps
      description: |
        Go onto the Internet. Find 10 maps that are interesting to you and do a writeup, in Markdown, in a file named: `10-interesting-maps.md`

        For each map you find:

        1. Write a short paragraph about why the map is interesting to you.
        2. Include the URL
        3. Take a screengrab, upload it somewhere online (such as imgur), and include it, e.g. using this kind of markup -

                 ![image caption here](http://example.com/image.jpg)

    - title: Read these essays about maps
      description: |
        Just read these, you don't have to write about them, but we will be discussing them in class:

        - [Map: Where are the gun permits in your neighborhood?](http://archive.lohud.com/interactive/article/20121223/NEWS01/121221011/Map-Where-gun-permits-your-neighborhood-)
        - [Gun permit data wasn’t maximized](http://www.cjr.org/behind_the_news/susan_mcgregor_on_gun_permit_d.php?page=all)
        - [When maps shouldn't be maps](http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/)

    - title: Look over my comments on your data beat memos
      description: I'll read your memos over the weekend and have some comments and suggestions on them. We'll continue to revise these over the next week.


# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-07": # Maps and lists
  topics:
    - Why maps work
    - Why maps don't work
    - Introduction to Fusion Tables and TileMill

  homework:
    - title: Map SFPD crimes
      description: |
        Using a snapshot of the [SFPD incident reports in 2014](https://docs.google.com/a/stanford.edu/spreadsheets/d/1tSc2ivROr9Gj6h27wU5THgPmiA3VrcfSPNVD_ravLqE/), repeat the steps in this [Fusion Tables tutorial](/tutorials/mapping/basic-fusion-tables), but map a different category of crime and make an interesting differentiation of the markers.

        Then publish your work in your __Github Pages repository__ as a file named, `sfpd-fusion-tables-map.html`. In other words, when I visit `http://YOUR_USERNAME.github.io/sfpd-fusion-tables-map`, I should be able to see your work.

    - title: Map gun-related homicides
      description: |
        Using  [TileMill](/tutorials/mapping/basic-fusion-tables), take the [Slate's collection of gun-related deaths for 2013](https://docs.google.com/a/stanford.edu/spreadsheets/d/1DLEnoeN-OYDYw6EfwGqX8HnrlnZgJT72zGfQZ3KI3Po/edit#gid=0), group the deaths by location, and map the data so that locations with more deaths will have different/bigger markers.

        Hints:

        1. You will have to use Pivot Tables for this. Think about the bare minimum of data you need to get the location data for the mapping tool and for the __count__ of deaths.
        2. You will most likely have to copy the Pivot Table and paste (or rather, **Paste special > Paste values only** in Google Spreadsheets) into a new spreadsheet and do some slight cleaning up before you import it into Fusion Tables or TileMill
        3. In your __private Github repo__, produce a Markdown file named `homicides-map.md`.
        4. Write a short paragraph explaining the deficencies of this kind of map visualization (think about what correlates with more shooting deaths). Include a screenshot of the map you made from TileMill.

        Note: If you have problems installing TileMill on your own computer and if I'm not able to get it on the lab computers, then you can do this assignment with Fusion Tables.

    - title: Sign up for the NICAR-L mailing list
      description: |
        This is the best discussion group for computer-assisted reporters. Talk about anything from technical questions to the state of the industry. [Sign up instructions are here](http://www.ire.org/resource-center/listservs/subscribe-nicar-l/). Basically, send an email to `listserv@lists.missouri.edu` and in the body of the message, enter:

                    SUBSCRIBE NICAR-L first_name last_name

        If you feel like the flow of emails is too much (it's best viewed in GMail), then send an email to `listserv@lists.missouri.edu` and in the **body** of the message (not the subject), enter:

                    SET NICAR-L DIGEST

          To go back to getting emails one-by-one, email that same address with the body of:

                    SET NICAR-L NODIGEST

          I personally set it to `NODIGEST` and then in my GMail filters, have all messages __From__ `NICAR-L@po.missouri.edu` be sent to archive to keep my inbox clean.

    - title: Sign up for a StackOverflow account
      description: |
        Hands down the best place to ask questions related to code. When you Google error messages or technical issues, you will almost always end up on a [StackOverflow page](http://stackoverflow.com/). Create an account so you can ask questions directly.

    - title: Join the OpenData StackExchange community
      description: |
        The [OpenData StackExchange site](http://opendata.stackexchange.com/) is a StackOverflow-type site devoted to questions about data, including where to get and find it. If you've created a StackOverflow account, you should be able to use the same login credentials for that as for [the OpenData site](http://opendata.stackexchange.com/).


# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-09": # Maps as illustrations
  topics:
    - Working with KML files
    - Intensity maps
    - Visual joins and intersections
  homework:
    - title: A map of political change in the United States
      description: |
        In class, we saw at least two examples of using the United States map to illustrate the prevalence of policy: BuzzFeed's [investigation into failure-to-protect laws](http://www.buzzfeed.com/alexcampbell/how-the-law-turns-battered-women-into-criminals#3wn6ef7) and [NYT's look at restrictions on gun permits](http://www.nytimes.com/interactive/2011/12/27/us/easing-restrictions-on-gun-permits.html?ref=us).

        Check out this tutorial: [Fusion Tables Intensity Maps with Custom Shapes](/tutorials/mapping/fusion-table-shapes/)

        And then make your own U.S. policy map. It can be like BuzzFeed's, in which there are several colors for the several categories of severity. Or, like NYT's, which does a side-by-side of two shaded U.S. maps showing how much policy has changed.

        First, pick a policy. One example would be: states that allow gay marriage today versus 10 years ago. But don't do that, because that's been done plenty of times in the past week.

        Then, make the map, which will involve [making a spreadsheet similar to BuzzFeed's example](https://s3.amazonaws.com/buzzfeed-media/Images/2014/09/buzzfeednews_failuretoprotectlaws.pdf) and then importing it into Fusion Tables. Or, you could try TileMill, in which case, you would handcode the MSS styles as appropriate. Whatever you feel most comfortable with.

        This is not really a test of your mapping skills, as the [state-level KML data is provided for you](/tutorials/mapping/fusion-table-shapes/). Instead, it's an exercise in reporting and organizing your notes. The fact that you organize it in a spreadsheet then makes it trivial to turn into an illustrative chart.


    - title: Install Sequel Pro or SQLite Manager (optional)
      description: |
        Time for some database fun. The lab computers have Sequel Pro installed. If you want to do database work at home, it's up to you to install this software on your own machine. [Here's a guide to get you started](/tutorials/databases/getting-around-sqlite-and-sequel-pro).

        **Note:** You should probably at least try getting SQLite Manager to work on your own computer. It's fairly easy to get up and running, and it lets you build datamaps with the SQLite format (which you might want to later on).

# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-14":
  topics:
    - MySQL / SQLite
    - Select, group, and aggregate
    - Where conditionals
    - SFPD reports of larceny, narcotics, and prostitution
    - Babies, and what we name them
  homework:
    - title: Read the SQL tutorials
      description: |
        I've put together a [series of step-by-step tutorials on basic SQL syntax](/tutorials#databases). Read through them and repeat every exercise if necessary. Don't just copy and paste; sometimes, typing out the queries forces you to slow down and think them through.
    - title: Investigate your own name
      description: |
        Using the Social Security Administration's database of baby names, find out how popular (or unpopular) your name has been since 1980 (or 1880, if you wish).

        Download the appropriate file for your system. You may want to try the 1980 file if your computer is having trouble with the full 1880 file.

        #### SQLite
        - [Names since 1980](http://stash.padjo.org/dumps/sql/ssa-babynames-since-1980.sqlite.zip)
        - [Names since 1880](http://stash.padjo.org/dumps/sql/ssa-babynames.sqlite.zip)

        #### SQL
        - [Names since 1980](http://stash.padjo.org/dumps/sql/ssa-babynames-since-1980.sql.zip)
        - [Names since 1880](http://stash.padjo.org/dumps/sql/ssa-babynames.sql.zip)

        To get some inspiration, you can check out the [SSA baby names website in which they let you perform a limited search on names](http://www.ssa.gov/oact/babynames/).

        Please do this homework in a Markdown file in your **private Github repo** and call it, `sql-baby-names.md`

        #### Using the `ssa_baby_names` table

        1. In the year 2013, find out how many babies had your name
        2. Find the highest-rank (and the year) that your name has achieved among baby names
        3. Between the years 1980 and 2013, find how many babies in total have been listed by the Social Security Administration in this data.
        4. Find out how many babies (roughly) have had your name from 1980 to 2013
        5. Find the year that had the most male babies born
        6. Find the year in which your name had the highest *increase* in names-per-100k-babies born
        7. Find the year in which your name had the highest *decrease* in names-per-100k-babies born
        8. Make a line chart showing how your name has changed in popularity over the years
        9. Find out who in our class had the most popular baby name in 1980.
        10. Find out who in our class had the most popular baby name in 2013.
        11. Find out who in our class has the name that the most babies throughout U.S. history have.

        #### Using the `ssa_baby_names_by_state` table

        For this section, pick a adoptive state if you were not born in the U.S.

        1. In your home state, find out how many babies had your name in the year that you were born.
        2. In your home state, find out how many babies had your name in 2013
        3. Find the state in which your name is the most popular in 2013
        4. Find the state in which your name is least popular (but still registers, i.e. has a minimum of 5 babies) in 2013
        5. Make a line chart with two lines:

           1. The popularity of your name in your home state
           2. The popularity of your name in California

           (If you were born in California, pick another state, like Florida.)



# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-16": # inner joins
  homework:
    - title: "Map your name"
      description: |
        Create at least **two maps** of the United States and the popularity of your name. [Kind of like this](http://jezebel.com/map-sixty-years-of-the-most-popular-names-for-girls-s-1443501909), except be more informational:

        ![img from jezebel](/files/lectures/2014-10-16/mary-1960-jezebel.png)



        By *more informational*, I mean for you to do something interesting with the level of granular data you have in the SSA database from the previous assignment ([MySQL](http://stash.padjo.org/dumps/sql/ssa-babynames-since-1980.sqlite.zip), [SQLite](http://stash.padjo.org/dumps/sql/ssa-babynames-since-1980.sqlite.zip))

        For this assignment, you will re-use the state KML data you've worked with before, [which can be found here](https://docs.google.com/a/stanford.edu/spreadsheets/d/18YaI4CDDOnhBHtX1vykmhwukfMeYqN9qdDggcnSfHFo/edit#gid=764026114).

        Remember how you had to match data to each state row by hand? Doing that, even just 50 times, is for suckers. Use a SQL inner join to join the **ssa_baby_names_by_state** table to the state KML spreadsheet, do some kind of aggregation at the state level, then export it out to Fusion Table or TileMill.

        What kind of aggregation? I leave it to you. One could be to shade the states by relative popularity of your name in 1983 and 2013. Or you could use the political vote data that's in the [provided KML spreadsheet](https://docs.google.com/a/stanford.edu/spreadsheets/d/18YaI4CDDOnhBHtX1vykmhwukfMeYqN9qdDggcnSfHFo/edit#gid=764026114), and see if there's a correlation between political leanings of a state and the popularity of your name. Knock yourself out.

        Why two maps (i.e. two Fusion Tables)? Because if you are using SQL and joins, you can create the intersection of the baby-names table and the KML table very quickly. If you are doing it by hand...well, not so much.

        The first step you will have to do is to export the KML-spreadsheet as CSV and then import it **into the database that contains the ssa_baby_names tables**. I have not made a tutorial that lists out those steps, which differ between **SQLite Manager** and **MySQL** (SQLite Manager is especially a pain). This tutorial by [Troy Thibodeaux ](https://github.com/tthibo/SQL-Tutorial/blob/master/tutorial_files/part1.textile) will help, but google around for more information.







  topics:
    - Inner joins
    - One-to-one relationships
    - Our politicians and what they tweet



# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-21": # haystacks without needles
  topics:
    - Left joins
    - NULL values
    - Which Congressmembers like Ellen Degeneres?

  homework:
    - title: Congress tweeting about Ebola
      description: |
        1. Create a Markdown file named: `congress-tweets-about-ebola.md`
        2. Download the Congress twitter data I've provided in the tutorials
          - [MySQL dump of Congress and Twitter data](http://stash.padjo.org/dumps/sql/congress_twitter.sql.zip)
          - [SQLite dump of Congress and Twitter data](http://stash.padjo.org/dumps/sql/congress_twitter.sqlite.zip)
        3. Create a time-series chart (i.e. a column or line chart) or a table showing the number of tweets that mention "Ebola" from Democrats and Republicans.

    - title: Read up on the LESO 1033 Program
      description: |
        Next class will involve a "midterm project" that will test your knowledge of spreadsheets, SQL, mapping, math, and web publishing. The topic will be the Department of Defense's surplus equipment program. Read up on the subject, start thinking of interesting queries you might want to do based on what you surmise from these data stories.

        - [MRAPs And Bayonets: What We Know About The Pentagon's 1033 Program](http://www.npr.org/2014/09/02/342494225/mraps-and-bayonets-what-we-know-about-the-pentagons-1033-program)
        - [Mapping the Spread of Military Gear](http://www.nytimes.com/interactive/2014/08/15/us/surplus-military-equipment-map.html)
        - [The Flow of Money and Equipment To Local Police](http://www.nytimes.com/interactive/2014/08/23/us/flow-of-money-and-equipment-to-local-police.html)
        - [MuckRock's FOIAs of 1033 Data](https://www.muckrock.com/news/archives/2014/sep/04/we-have-over-half-countrys-agency-agency-1033-data/)

# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-23": # midterm
  topics:
    - A midterm on SQL and data
    - Data on military surplus distributed to U.S. counties
    - U.S. Census QuickFacts
  homework:
    - title: Do the Midterm
      duedate: 2014-10-30
      description: |
        1. Create a file named `midterm.md` in your private Github repo
        2. Answer the question in the lecture notes.


# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-28": # Election data
  topics:
    - Polling and pollsters
    - Following the campaign finance money
    - Competitive U.S. Senate races
  homework:
    - title: Analyze a Senate race's campaign spending
      description: |
        Teams (groups of 3) will each focus on one of the 7 most highly-contested Senate races and produce a data dossier that includes:

        - An analysis of campaign finance records for the two current candidates, including how much money has been spent, where the money came from, and how much money is currently on hand.
        - Repeat the analysis for the 2008 race and compare the difference with 2014.
    - title: Analyze polling methodologies
      description: |
        Read the methodologies for the Upshot's Leo, FiveThirtyEight, WaPo's Election Lab, and HuffPo's Pollster


# ----------------------------# ------------# ----------------------------# ------------------



"2014-10-30":
  topics:
    - Statistical significance
    - Election historical data
    - Past presidential results by county
    - Past U.S. Senate results by county
  homework:
    - title: Senate prediction pool
      description: Each group will guess the margin of error for each of the highly-contested Senate races. Winners get something TBA.

# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-04":
  topics:
    - No class because of Election Day Coverage


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-06":
  topics:
    - Avoiding chartjunk
    - Color sensibility


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-11":
  topics:
    - Correlation and causation
    - Simpson's Paradox
    - Statistical significance


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-13":
  topics:
    - Yelp reviews and restaurant inspections


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-18":
  topics:
    - How social media sites measure us


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-20":


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-25":


# ----------------------------# ------------# ----------------------------# ------------------


"2014-11-27":


# ----------------------------# ------------# ----------------------------# ------------------


"2014-12-02":


# ----------------------------# ------------# ----------------------------# ------------------


"2014-12-04":
